package=ai.vespa.embedding.config

# GGUF embedding model
embeddingModel model

# Maximum number of model layers to run on GPU (default: 0 = CPU-only).
gpuLayers int default=0

# Set pooling type for embeddings (default: UNSPECIFIED, UNSPECIFIED = model default).
poolingType enum {UNSPECIFIED, NONE, MEAN, CLS, LAST, RANK} default=UNSPECIFIED

# Set the physical maximum batch size (default: -1 = use runtime default).
physicalMaxBatchSize int default=-1

# Set the logical maximum batch size (default: -1 = use runtime default).
logicalMaxBatchSize int default=-1

# Continuous batching mode (default: false).
continuousBatching bool default=false

# Set the size of the prompt context (default: 0 = loaded from model).
contextSize int default=0

# Set the number of tokens to keep from the initial prompt (default: 0 = all)
maxPromptTokens int default=0

# Random number generator seed used by LLM runtime during inference (default: -1 = use random seed).
seed int default = -1

# Set the maximum number of parallel sequences to decode (default: 1 = no parallel decoding).
parallel int default=1

# Set the number of threads to use during generation (default: 0 = use runtime default).
# - If 0>, the the number of threads as an absolute value.
# - If 0, use runtime default.
# - If <0, the number of threads as a ratio of the cpu cores, e.g. -1 for all cores.
threads double default=0

# Set the number of threads to use during batch and prompt processing (default: 0 = use runtime default).
# - If 0>, the the number of threads as an absolute value.
# - If 0, use runtime default.
# - If <0, the number of threads as a ratio of the cpu cores, e.g. -1 for all cores.
batchThreads double default=0

# Prefix to prepend to prompt (if in query context)
prependQuery string default=""

# Prefix to prepend to prompt (if in document context)
prependDocument string default=""

# Normalize output embeddings
normalize bool default=false
